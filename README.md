# MeCo

<p align="left">
  <a href="https://arxiv.org/abs/2503.09027">
    <img src="https://img.shields.io/badge/arXiv-2503.09027-b31b1b.svg" alt="arXiv">
  </a>
</p>

This repository maintains the official implementation of the paper **Measure Twice, Cut Once: Grasping Video Structures and Event Semantics with LLMs for Video Temporal Localization**

## ğŸ“‹ To-Do List
- [x] Upload code
- [ ] Upload data  
- [ ] Upload checkpoint

## ğŸ› ï¸ Installation

Please refer to the following environmental settings that we use. You may install these packages by yourself if you meet any problem during automatic installation.

- CUDA 11.8
- Python 3.12.2
- PyTorch 2.1.2
- [Transformers](https://github.com/huggingface/transformers) 4.44.2
- [DeepSpeed](https://github.com/microsoft/DeepSpeed) 0.14.5
- [NNCore](https://github.com/yeliudev/nncore) 0.4.5

### Install from source

1. Clone the repository from GitHub.

```shell
git clone https://github.com/pangzss/MeCo.git
cd MeCo
```

2. Initialize conda environment.

```shell
conda create -n meco python=3.12 -y
conda activate meco
```

3. Install dependencies.

```shell
pip install -r requirements.txt
```

## ğŸš€ Getting Started

Following [ETChat](https://github.com/PolyU-ChenLab/ETBench/tree/main), we apply a three-stage training recipe for MeCo, where the first stage is for modality alignment, the second stage is for acquiring general chatting abilities, and the third stage is for enhancing time-sensitive entity comprehension abilities.

### Prepare model checkpoints

We compare the learnable modules in each stage, and provide their checkpoints as follows.

|| Encoder | Q-Former | Aggregator | Projector | LLM (LoRA) | Checkpoint |
|-|:-:|:-:|:-:|:-:|:-:|:-:|
| `Stage-1` | â„ï¸ | â„ï¸ | ğŸ”¥ | ğŸ”¥ | â„ï¸ | [4B](TBD) / [7B](TBD) |
| `Stage-2` | â„ï¸ | ğŸ”¥ | ğŸ”¥ | ğŸ”¥ | ğŸ”¥ | [4B](TBD) / [7B](TBD) |
| `Stage-3` | â„ï¸ | ğŸ”¥ / â„ï¸ | ğŸ”¥ | ğŸ”¥ | ğŸ”¥ | [4B](TBD) / [7B](TBD) |

If you want to start from `stage-1`, the pre-trained weights from [Phi3-Mini-4K-Instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct), [MiniCPM-V-2_6_QWen2](TBD), [EVA-ViT-G](https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/BLIP2/eva_vit_g.pth), and [Q-Former](https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/InstructBLIP/instruct_blip_vicuna7b_trimmed.pth) are required for initializing the model. The downloaded checkpoints shall be saved in the `checkpoints` folder.

### Training datasets

The training data used in each stage follows a similar structure to E.T. Chat. We use the ET-Instruct-164K dataset for stage-3 training with additional entity comprehension annotations.

|| Video Data | Image Data | Annotations |
|-|-|-|-|
| `Stage-1` | [WebVid](https://maxbain.com/webvid-dataset/) | [LCS-558K](https://huggingface.co/datasets/liuhaotian/LLaVA-Pretrain) | [llava_558k_with_webvid.json](https://huggingface.co/datasets/YanweiLi/LLaMA-VID-Data/resolve/main/llava_558k_with_webvid.json) |
| `Stage-2` | [ActivityNet](http://activity-net.org/download.html) / [VideoChatGPT](https://mbzuaiac-my.sharepoint.com/:f:/g/personal/hanoona_bangalath_mbzuai_ac_ae/EnLRDehrr8lGqHpC5w1zZ9QBnsiVffYy5vCv8Hl14deRcg?e=Ul5DUE) | [LLaVA-1.5-Instruct](https://github.com/haotian-liu/LLaVA?tab=readme-ov-file#visual-instruction-tuning) | [llava_v1_5_mix665k_with_video_chatgpt.json](https://huggingface.co/datasets/YanweiLi/LLaMA-VID-Data/resolve/main/llava_v1_5_mix665k_with_video_chatgpt.json) |
| `Stage-3` | [ET-Instruct-164K](https://huggingface.co/datasets/PolyU-ChenLab/ET-Instruct-164K) | - | [et_instruct_164k_meco.json](TBD) |

Download the required datasets and place them in the `data` folder. It is strongly recommended to compress the videos (to `3 FPS` & `224px`) using the [script](tools/compress_videos.py) provided. After processing, make sure the files are organized in the following structure.

```
MeCo
â”œâ”€ data
â”‚  â”œâ”€ llamavid
â”‚  â”‚  â”œâ”€ llava_558k_with_webvid.json
â”‚  â”‚  â””â”€ llava_v1_5_mix665k_with_video_chatgpt.json
â”‚  â”œâ”€ llava_pretrain                 â”€â”
â”‚  â”‚  â””â”€ images                       â”‚ For
â”‚  â”œâ”€ webvid                          â”‚ Stage-1
â”‚  â”‚  â””â”€ videos                      â”€â”˜
â”‚  â”œâ”€ llava_instruct                 â”€â”
â”‚  â”‚  â”œâ”€ coco                         â”‚
â”‚  â”‚  â”œâ”€ gqa                          â”‚
â”‚  â”‚  â”œâ”€ ocr_vqa                      â”‚ For
â”‚  â”‚  â”œâ”€ textvqa                      â”‚ Stage-2
â”‚  â”‚  â””â”€ vg                           â”‚
â”‚  â”œâ”€ video_chatgpt                   â”‚
â”‚  â”‚  â””â”€ activitynet                 â”€â”˜
â”‚  â”œâ”€ ET-Instruct-164K               â”€â”
â”‚  â”‚  â”œâ”€ videos                       â”‚
â”‚  â”‚  â”œâ”€ et_instruct_164k_txt.json    â”‚ For
â”‚  â”‚  â”œâ”€ et_instruct_164k_vid.json    â”‚ Stage-3
â”‚  â”‚  â””â”€ et_instruct_164k_meco.json  â”€â”˜
â”œâ”€ checkpoints
â”‚  â”œâ”€ Phi-3-mini-4k-instruct
â”‚  â”œâ”€ MiniCPM-V-2_6_QWen2
â”‚  â”œâ”€ eva_vit_g.pth
â”‚  â””â”€ instruct_blip_vicuna7b_trimmed.pth
â”œâ”€ meco
â”œâ”€ scripts
â””â”€ README.md
```
### Benchmarks
Please follow [ETBench](https://github.com/PolyU-ChenLab/ETBench) to download the ETBench benchmark, and follow [R2-Tuning](https://github.com/yeliudev/R2-Tuning) for the preparations of the Charades-STA and QVHighlights (QVH) benchmarks. The downloaded datasets should be placed in the `data` folder as well.

## ğŸ”® Training

Use the following commands to train MeCo. The default setting is to use 4 GPUs. You may modify `nproc_per_node`, `per_device_train_batch_size`, and `gradient_accumulation_steps` to keep the same global batch size if you have different device configurations.

```shell
# Stage-1
bash scripts/train_stage_1_phi3.sh # 4B
bash scripts/train_stage_1_minicpmv_qwen2.sh # 7B
# Stage-2
bash scripts/train_stage_2_phi3.sh [<path-to-stage-1-checkpoint>]
bash scripts/train_stage_2_minicpmv_qwen2.sh [<path-to-stage-1-checkpoint>]
# Stage-3
bash scripts/train_stage_3_phi3.sh [<path-to-stage-2-checkpoint>]
bash scripts/train_stage_3_minicpmv_qwen2.sh [<path-to-stage-2-checkpoint>]
```

The training logs and checkpoints will be saved in the `work_dirs` folder.

## ğŸ’» Inference

Use the following commands to run inference on different benchmarks.

### E.T. Bench Evaluation

```shell
bash scripts/inference_etbench.sh [<path-to-checkpoint>] [<task>] [<pred-dir>] [<dtype>]
```

### Charades-STA QVH Evaluation

```shell
bash scripts/inference_charade_qvh.sh [<anno-path>] [<data-path>] [<path-to-checkpoint>] [<pred-path>] [<dtype>]
```

This will generate prediction files that can be evaluated using the corresponding evaluation scripts in the `tools` folder.

After running inference, you can compute evaluation metrics using the provided scripts.

### E.T. Bench Metrics

```shell
# Compute metrics for E.T. Bench predictions
python tools/compute_metrics.py <path-to-prediction-file-or-dir>
```

### Charades-STA QVH Metrics

```shell
# Compute metrics for Charades-STA QVH predictions  
python tools/compute_metrics_charades_qvh.py <path-to-prediction-file-or-dir>
```
## ğŸ™ Acknowledgments

This README is adapted from the [E.T. Chat documentation](https://github.com/PolyU-ChenLab/ETBench/blob/main/docs/MODEL.md).

This work is built upon the excellent foundations of:
- [E.T. Chat](https://github.com/PolyU-ChenLab/ETBench)
- [R2-Tuning](https://github.com/yeliudev/R2-Tuning)
- [LLaVA](https://github.com/haotian-liu/LLaVA)
- [MomentDETR](https://github.com/jayleicn/moment_detr)
